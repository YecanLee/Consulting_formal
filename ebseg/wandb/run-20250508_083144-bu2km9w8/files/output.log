[32m[05/08 08:31:51 d2.engine.defaults]: [0mModel:
EBSeg(
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 1.5, '1_loss_ce': 1.5, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 1.5, '1_loss_ce_0': 1.5, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 1.5, '1_loss_ce_1': 1.5, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 1.5, '1_loss_ce_2': 1.5, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 1.5, '1_loss_ce_3': 1.5, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 1.5, '1_loss_ce_4': 1.5, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 1.5, '1_loss_ce_5': 1.5, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 1.5, '1_loss_ce_6': 1.5, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 1.5, '1_loss_ce_7': 1.5, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 1.5, '1_loss_ce_8': 1.5, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'mse_ssc_loss_1': 10, 'mse_ssc_loss_2': 10, 'mse_ssc_loss_3': 10, 'mse_ssc_loss_4': 10, 'mse_ssc_loss_5': 10, 'mse_ssc_loss_6': 10, 'mse_ssc_loss_7': 10, 'mse_ssc_loss_8': 10, 'mse_ssc_loss_9': 10, 'mse_ssc_loss_10': 10, 'mse_ssc_loss_11': 10, 'mse_ssc_loss_12': 10, 'mse_ssc_loss_13': 10, 'mse_ssc_loss_14': 10, 'mse_ssc_loss_15': 10, 'mse_ssc_loss_16': 10, 'mse_ssc_loss_17': 10, 'mse_ssc_loss_18': 10, 'mse_ssc_loss_19': 10, 'mse_ssc_loss_20': 10}
      num_classes: 171
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (clip_visual_extractor): CLIP_surgery_FeatureExtractor(
    (patchnorm_pre_ln): Identity()
    (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (resblocks): ModuleList(
      (0-3): 4 x ResidualAttentionBlock(
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ls_2): Identity()
      )
      (4-17): 14 x Surgery_ResidualAttentionBlock(
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (clip_rec_head): CLIP_surgery_RecWithAttnbiasHead(
    (resblocks): ModuleList(
      (0-5): 6 x Surgery_ResidualAttentionBlock(
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (ov_classifier): LearnableBgOvClassifier(
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0-11): 12 x ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (token_embedding): Embedding(49408, 768)
    (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (coco_text_embedding_distance): PairwiseDistance()
  )
  (backbone): ImageEncoderViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    )
    (blocks): ModuleList(
      (0-11): 12 x Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (lin1): Linear(in_features=768, out_features=3072, bias=True)
          (lin2): Linear(in_features=3072, out_features=768, bias=True)
          (act): GELU(approximate='none')
        )
      )
    )
  )
  (sem_seg_head): EBSeg_Mask2former_Head(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0-2): 3 x Sequential(
          (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (pool_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (mask_embed1): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=768, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (attn_proj): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4096, bias=True)
        )
      )
      (bias_scaling): Linear(in_features=1, out_features=1, bias=True)
    )
    (clip_fusion_layers): ModuleList(
      (0-2): 3 x Sequential(
        (0): LayerNorm()
        (1): Conv2d(1024, 768, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
  (ssc_mse_loss): MSELoss()
)
[5m[31mWARNING[0m [32m[05/08 08:31:51 d2.data.datasets.coco]: [0m
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[32m[05/08 08:31:51 d2.data.datasets.coco]: [0mLoaded 118 images in COCO format from /home/ra78lof/consulting_pro/SAN/san/data/ceiling_painting_segmentation/train/json_annotation_train.json
Processed 118 records with semantic segmentation masks for ceiling_easy_train.
[32m[05/08 08:31:51 d2.data.build]: [0mRemoved 22 images with no usable annotations. 96 images left.
[32m[05/08 08:31:51 d2.data.build]: [0mDistribution of instances among all 4 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|   mural    | 0            |   brief    | 2            |   mural    | 338          |
|   relief   | 3            |            |              |            |              |
|   total    | 343          |            |              |            |              |[0m
[32m[05/08 08:31:51 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in training: [RandomCrop(crop_type='absolute', crop_size=[640, 640]), ResizeShortestEdge(short_edge_length=(320, 384, 448, 512, 576, 640, 704, 768, 832, 896, 960), max_size=2560, sample_style='choice'), RandomFlip()]
[32m[05/08 08:31:51 ebseg.data.build]: [0mUsing training sampler TrainingSampler
[32m[05/08 08:31:51 d2.data.common]: [0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[32m[05/08 08:31:51 d2.data.common]: [0mSerializing 96 elements to byte tensors and concatenating them all ...
[32m[05/08 08:31:51 d2.data.common]: [0mSerialized dataset takes 151.07 MiB
[32m[05/08 08:31:51 d2.data.build]: [0mMaking batched data loader with batch_size=1
[32m[05/08 08:31:51 d2.checkpoint.detection_checkpoint]: [0m[DetectionCheckpointer] Loading from  ...
[32m[05/08 08:31:51 fvcore.common.checkpoint]: [0mNo checkpoint found. Initializing model from scratch
[32m[05/08 08:31:51 d2.engine.train_loop]: [0mStarting training from iteration 0
/home/ra78lof/anaconda3/envs/ebseg/lib/python3.11/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
[4m[5m[31mERROR[0m [32m[05/08 08:31:52 d2.engine.train_loop]: [0mException during training:
Traceback (most recent call last):
  File "/home/ra78lof/consulting_pro/detectron2/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/ra78lof/consulting_pro/detectron2/detectron2/engine/defaults.py", line 530, in run_step
    self._trainer.run_step()
  File "/home/ra78lof/consulting_pro/detectron2/detectron2/engine/train_loop.py", line 494, in run_step
    loss_dict = self.model(data)
                ^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/ebseg/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/ebseg/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/consulting_pro/ebseg/ebseg/model/EBSeg.py", line 224, in forward
    outputs_head = self.sem_seg_head(image_features, clip_image_features=image_features_clip)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/ebseg/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/ebseg/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/consulting_pro/ebseg/ebseg/model/mask2former/modeling/meta_arch/ebseg_mask2former_head.py", line 103, in forward
    predictions = self.predictor(multi_scale_features, mask_features, mask)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/ebseg/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/ebseg/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/consulting_pro/ebseg/ebseg/model/mask2former/modeling/transformer_decoder/transformer_decoder.py", line 395, in forward
    attn_bias, mask_embed, outputs_mask, attn_mask = self.forward_prediction_heads(output, mask_features, attn_mask_target_size=size_list[(i + 1) % self.num_feature_levels],mask_features_for_attn_bias=mask_features_for_attn_bias)
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/consulting_pro/ebseg/ebseg/model/mask2former/modeling/transformer_decoder/transformer_decoder.py", line 413, in forward_prediction_heads
    attn_bias = torch.einsum("bqc,blnchw->blnqhw", mask_embed, mask_features_for_attn_bias)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/ebseg/lib/python3.11/site-packages/torch/functional.py", line 385, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 176.00 MiB. GPU
[32m[05/08 08:31:52 d2.engine.hooks]: [0mTotal training time: 0:00:01 (0:00:00 on hooks)
[32m[05/08 08:31:52 d2.utils.events]: [0m iter: 0       lr: N/A  max_mem: 23344M
