MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
train:
  batch_size_per_gpu: 2
  dataset_path: Custom:../../ceiling_dataset_for_ViT-P
  output_dir: /home/ra78lof/consulting_pro/ViT-P/ViT-P/OUTPUT_DIR
  saveckp_freq: 20
  seed: 0
  num_workers: 32
  OFFICIAL_EPOCH_LENGTH: 5
  cache_dataset: false
student:
  arch: vit_large
  num_points: 250
  num_classes: 4
  patch_size: 14
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ./dinov2_vitl14_pretrain.pth
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
optim:
  epochs: 30
  weight_decay: 0
  lr: 0.01
  warmup_epochs: 0
  clip_grad: 1.0
  scaling_rule: sqrt_wrt_1024
crops:
  global_crops_size:
  - 518
  - 518
evaluation:
  eval_period_iterations: 500
