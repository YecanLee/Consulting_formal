[05/15 07:27:32] detectron2 INFO: Rank of current process: 0. World size: 1
[05/15 07:27:32] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0]
numpy                            2.2.5
detectron2                       0.6 @/home/ra78lof/consulting_pro/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.0
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   535.171.04
CUDA_HOME                        /usr
Pillow                           11.2.1
torchvision                      0.19.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.11.0
-------------------------------  ------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[05/15 07:27:32] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:57897', opts=[])
[05/15 07:27:32] detectron2 INFO: Contents of args.config_file=configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml:
_BASE_: Base-COCO-PanopticSegmentation.yaml
MODEL:
  MASK_ON: True
  WEIGHTS: "./mask_proposal_network.pth"
  META_ARCHITECTURE: "MaskFormer"
  SEM_SEG_HEAD:
    NAME: "MaskFormerHead"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 4
    LOSS_WEIGHT: 1.0
    CONVS_DIM: 256
    MASK_DIM: 256
    NORM: "GN"
    # pixel decoder
    PIXEL_DECODER_NAME: "MSDeformAttnPixelDecoder"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES: ["res3", "res4", "res5"]
    COMMON_STRIDE: 4
    TRANSFORMER_ENC_LAYERS: 6
  MASK_FORMER:
    TRANSFORMER_DECODER_NAME: "MultiScaleMaskedTransformerDecoder"
    TRANSFORMER_IN_FEATURE: "multi_scale_pixel_decoder"
    DEEP_SUPERVISION: True
    NO_OBJECT_WEIGHT: 0.0 
    CLASS_WEIGHT: 2.0
    MASK_WEIGHT: 5.0
    DICE_WEIGHT: 5.0
    HIDDEN_DIM: 256
    NUM_OBJECT_QUERIES: 100
    NHEADS: 8
    DROPOUT: 0.0
    DIM_FEEDFORWARD: 2048
    ENC_LAYERS: 0
    PRE_NORM: False
    ENFORCE_INPUT_PROJ: False
    SIZE_DIVISIBILITY: 32
    DEC_LAYERS: 10  # 9 decoder layers, add one for the loss on learnable query
    TRAIN_NUM_POINTS: 12544
    OVERSAMPLE_RATIO: 3.0
    IMPORTANCE_SAMPLE_RATIO: 0.75
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: False
      PANOPTIC_ON: False
      OVERLAP_THRESHOLD: 0.5
      OBJECT_MASK_THRESHOLD: 0.5
OUTPUT_DIR: ./output


[05/15 07:27:32] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - coco_2017_val_panoptic_with_sem_seg
  TRAIN:
  - coco_2017_train_panoptic
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_panoptic_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  CLIP_MODEL:
    HEADS: 16
    INPUT_RESOLUTION: 336
    LAYERS: 24
    NAME: ViT-L/14@336px
    OUTPUT_DIM: 768
    PATCH_SIZE: 14
    TEMPERATURE: 0.01
    WIDTH: 1024
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.0
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: false
      OBJECT_MASK_THRESHOLD: 0.5
      OVERLAP_THRESHOLD: 0.5
      PANOPTIC_ON: false
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: true
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 4
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: ./mask_proposal_network.pth
OUTPUT_DIR: ./output
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 10000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 368751
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[05/15 07:27:32] detectron2 INFO: Full config saved to ./output/config.yaml
[05/15 07:27:32] d2.utils.env INFO: Using a generated random seed 33633695
[05/15 07:27:48] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0}
      num_classes: 4
      eos_coef: 0.0
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (maskclip): MaskCLIP(
    (visual): VisionTransformer(
      (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
      (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (12): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (13): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (14): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (15): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (16): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (17): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (18): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (19): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (20): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (21): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (22): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (23): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (conv1_added_params): Sequential(
          (0): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (1): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (2): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (3): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
        )
        (conv3_added_params): Sequential(
          (0): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (3): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
[05/15 07:31:07] detectron2 INFO: Rank of current process: 0. World size: 1
[05/15 07:31:07] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0]
numpy                            2.2.5
detectron2                       0.6 @/home/ra78lof/consulting_pro/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.0
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   535.171.04
CUDA_HOME                        /usr
Pillow                           11.2.1
torchvision                      0.19.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.11.0
-------------------------------  ------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[05/15 07:31:07] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:57897', opts=[])
[05/15 07:31:07] detectron2 INFO: Contents of args.config_file=configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml:
_BASE_: Base-COCO-PanopticSegmentation.yaml
MODEL:
  MASK_ON: True
  WEIGHTS: "./mask_proposal_network.pth"
  META_ARCHITECTURE: "MaskFormer"
  SEM_SEG_HEAD:
    NAME: "MaskFormerHead"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 4
    LOSS_WEIGHT: 1.0
    CONVS_DIM: 256
    MASK_DIM: 256
    NORM: "GN"
    # pixel decoder
    PIXEL_DECODER_NAME: "MSDeformAttnPixelDecoder"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES: ["res3", "res4", "res5"]
    COMMON_STRIDE: 4
    TRANSFORMER_ENC_LAYERS: 6
  MASK_FORMER:
    TRANSFORMER_DECODER_NAME: "MultiScaleMaskedTransformerDecoder"
    TRANSFORMER_IN_FEATURE: "multi_scale_pixel_decoder"
    DEEP_SUPERVISION: True
    NO_OBJECT_WEIGHT: 0.0 
    CLASS_WEIGHT: 2.0
    MASK_WEIGHT: 5.0
    DICE_WEIGHT: 5.0
    HIDDEN_DIM: 256
    NUM_OBJECT_QUERIES: 100
    NHEADS: 8
    DROPOUT: 0.0
    DIM_FEEDFORWARD: 2048
    ENC_LAYERS: 0
    PRE_NORM: False
    ENFORCE_INPUT_PROJ: False
    SIZE_DIVISIBILITY: 32
    DEC_LAYERS: 10  # 9 decoder layers, add one for the loss on learnable query
    TRAIN_NUM_POINTS: 12544
    OVERSAMPLE_RATIO: 3.0
    IMPORTANCE_SAMPLE_RATIO: 0.75
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: False
      PANOPTIC_ON: False
      OVERLAP_THRESHOLD: 0.5
      OBJECT_MASK_THRESHOLD: 0.5
OUTPUT_DIR: ./output
DATASETS:
  TRAIN: ("ceiling_easy_train",)
  TEST: ("ceiling_easy_val",)
INPUT:
  DATASET_MAPPER_NAME: "ceiling_painting"

[05/15 07:31:07] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ceiling_easy_val
  TRAIN:
  - ceiling_easy_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: ceiling_painting
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  CLIP_MODEL:
    HEADS: 16
    INPUT_RESOLUTION: 336
    LAYERS: 24
    NAME: ViT-L/14@336px
    OUTPUT_DIM: 768
    PATCH_SIZE: 14
    TEMPERATURE: 0.01
    WIDTH: 1024
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.0
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: false
      OBJECT_MASK_THRESHOLD: 0.5
      OVERLAP_THRESHOLD: 0.5
      PANOPTIC_ON: false
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: true
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 4
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: ./mask_proposal_network.pth
OUTPUT_DIR: ./output
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 10000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 368751
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[05/15 07:31:07] detectron2 INFO: Full config saved to ./output/config.yaml
[05/15 07:31:07] d2.utils.env INFO: Using a generated random seed 8643823
[05/15 07:35:38] detectron2 INFO: Rank of current process: 0. World size: 1
[05/15 07:35:38] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0]
numpy                            2.2.5
detectron2                       0.6 @/home/ra78lof/consulting_pro/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.0
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   535.171.04
CUDA_HOME                        /usr
Pillow                           11.2.1
torchvision                      0.19.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.11.0
-------------------------------  ------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[05/15 07:35:38] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:57897', opts=[])
[05/15 07:35:38] detectron2 INFO: Contents of args.config_file=configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml:
_BASE_: Base-COCO-PanopticSegmentation.yaml
MODEL:
  MASK_ON: True
  WEIGHTS: "./mask_proposal_network.pth"
  META_ARCHITECTURE: "MaskFormer"
  SEM_SEG_HEAD:
    NAME: "MaskFormerHead"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 4
    LOSS_WEIGHT: 1.0
    CONVS_DIM: 256
    MASK_DIM: 256
    NORM: "GN"
    # pixel decoder
    PIXEL_DECODER_NAME: "MSDeformAttnPixelDecoder"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES: ["res3", "res4", "res5"]
    COMMON_STRIDE: 4
    TRANSFORMER_ENC_LAYERS: 6
  MASK_FORMER:
    TRANSFORMER_DECODER_NAME: "MultiScaleMaskedTransformerDecoder"
    TRANSFORMER_IN_FEATURE: "multi_scale_pixel_decoder"
    DEEP_SUPERVISION: True
    NO_OBJECT_WEIGHT: 0.0 
    CLASS_WEIGHT: 2.0
    MASK_WEIGHT: 5.0
    DICE_WEIGHT: 5.0
    HIDDEN_DIM: 256
    NUM_OBJECT_QUERIES: 100
    NHEADS: 8
    DROPOUT: 0.0
    DIM_FEEDFORWARD: 2048
    ENC_LAYERS: 0
    PRE_NORM: False
    ENFORCE_INPUT_PROJ: False
    SIZE_DIVISIBILITY: 32
    DEC_LAYERS: 10  # 9 decoder layers, add one for the loss on learnable query
    TRAIN_NUM_POINTS: 12544
    OVERSAMPLE_RATIO: 3.0
    IMPORTANCE_SAMPLE_RATIO: 0.75
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: False
      PANOPTIC_ON: False
      OVERLAP_THRESHOLD: 0.5
      OBJECT_MASK_THRESHOLD: 0.5
OUTPUT_DIR: ./output
DATASETS:
  TRAIN: ("ceiling_easy_train",)
  TEST: ("ceiling_easy_val",)
INPUT:
  DATASET_MAPPER_NAME: "ceiling_painting"

[05/15 07:35:38] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ceiling_easy_val
  TRAIN:
  - ceiling_easy_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: ceiling_painting
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  CLIP_MODEL:
    HEADS: 16
    INPUT_RESOLUTION: 336
    LAYERS: 24
    NAME: ViT-L/14@336px
    OUTPUT_DIM: 768
    PATCH_SIZE: 14
    TEMPERATURE: 0.01
    WIDTH: 1024
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.0
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: false
      OBJECT_MASK_THRESHOLD: 0.5
      OVERLAP_THRESHOLD: 0.5
      PANOPTIC_ON: false
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: true
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 4
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: ./mask_proposal_network.pth
OUTPUT_DIR: ./output
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 10000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 368751
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[05/15 07:35:38] detectron2 INFO: Full config saved to ./output/config.yaml
[05/15 07:35:38] d2.utils.env INFO: Using a generated random seed 39521812
[05/15 07:36:21] detectron2 INFO: Rank of current process: 0. World size: 1
[05/15 07:36:21] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0]
numpy                            2.2.5
detectron2                       0.6 @/home/ra78lof/consulting_pro/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.0
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   535.171.04
CUDA_HOME                        /usr
Pillow                           11.2.1
torchvision                      0.19.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.11.0
-------------------------------  ------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[05/15 07:36:21] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:57897', opts=[])
[05/15 07:36:21] detectron2 INFO: Contents of args.config_file=configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml:
_BASE_: Base-COCO-PanopticSegmentation.yaml
MODEL:
  MASK_ON: True
  WEIGHTS: "./mask_proposal_network.pth"
  META_ARCHITECTURE: "MaskFormer"
  SEM_SEG_HEAD:
    NAME: "MaskFormerHead"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 4
    LOSS_WEIGHT: 1.0
    CONVS_DIM: 256
    MASK_DIM: 256
    NORM: "GN"
    # pixel decoder
    PIXEL_DECODER_NAME: "MSDeformAttnPixelDecoder"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES: ["res3", "res4", "res5"]
    COMMON_STRIDE: 4
    TRANSFORMER_ENC_LAYERS: 6
  MASK_FORMER:
    TRANSFORMER_DECODER_NAME: "MultiScaleMaskedTransformerDecoder"
    TRANSFORMER_IN_FEATURE: "multi_scale_pixel_decoder"
    DEEP_SUPERVISION: True
    NO_OBJECT_WEIGHT: 0.0 
    CLASS_WEIGHT: 2.0
    MASK_WEIGHT: 5.0
    DICE_WEIGHT: 5.0
    HIDDEN_DIM: 256
    NUM_OBJECT_QUERIES: 100
    NHEADS: 8
    DROPOUT: 0.0
    DIM_FEEDFORWARD: 2048
    ENC_LAYERS: 0
    PRE_NORM: False
    ENFORCE_INPUT_PROJ: False
    SIZE_DIVISIBILITY: 32
    DEC_LAYERS: 10  # 9 decoder layers, add one for the loss on learnable query
    TRAIN_NUM_POINTS: 12544
    OVERSAMPLE_RATIO: 3.0
    IMPORTANCE_SAMPLE_RATIO: 0.75
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: False
      PANOPTIC_ON: False
      OVERLAP_THRESHOLD: 0.5
      OBJECT_MASK_THRESHOLD: 0.5
OUTPUT_DIR: ./output
DATASETS:
  TRAIN: ("ceiling_easy_train",)
  TEST: ("ceiling_easy_val",)
INPUT:
  DATASET_MAPPER_NAME: "ceiling_painting"

[05/15 07:36:21] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ceiling_easy_val
  TRAIN:
  - ceiling_easy_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: ceiling_painting
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  CLIP_MODEL:
    HEADS: 16
    INPUT_RESOLUTION: 336
    LAYERS: 24
    NAME: ViT-L/14@336px
    OUTPUT_DIM: 768
    PATCH_SIZE: 14
    TEMPERATURE: 0.01
    WIDTH: 1024
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.0
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: false
      OBJECT_MASK_THRESHOLD: 0.5
      OVERLAP_THRESHOLD: 0.5
      PANOPTIC_ON: false
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: true
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 4
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: ./mask_proposal_network.pth
OUTPUT_DIR: ./output
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 10000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 368751
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[05/15 07:36:21] detectron2 INFO: Full config saved to ./output/config.yaml
[05/15 07:36:21] d2.utils.env INFO: Using a generated random seed 22612406
[05/15 07:39:04] detectron2 INFO: Rank of current process: 0. World size: 1
[05/15 07:39:04] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0]
numpy                            2.2.5
detectron2                       0.6 @/home/ra78lof/consulting_pro/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.0
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   535.171.04
CUDA_HOME                        /usr
Pillow                           11.2.1
torchvision                      0.19.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.11.0
-------------------------------  ------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[05/15 07:39:04] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:57897', opts=[])
[05/15 07:39:04] detectron2 INFO: Contents of args.config_file=configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml:
_BASE_: Base-COCO-PanopticSegmentation.yaml
MODEL:
  MASK_ON: True
  WEIGHTS: "./mask_proposal_network.pth"
  META_ARCHITECTURE: "MaskFormer"
  SEM_SEG_HEAD:
    NAME: "MaskFormerHead"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 4
    LOSS_WEIGHT: 1.0
    CONVS_DIM: 256
    MASK_DIM: 256
    NORM: "GN"
    # pixel decoder
    PIXEL_DECODER_NAME: "MSDeformAttnPixelDecoder"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES: ["res3", "res4", "res5"]
    COMMON_STRIDE: 4
    TRANSFORMER_ENC_LAYERS: 6
  MASK_FORMER:
    TRANSFORMER_DECODER_NAME: "MultiScaleMaskedTransformerDecoder"
    TRANSFORMER_IN_FEATURE: "multi_scale_pixel_decoder"
    DEEP_SUPERVISION: True
    NO_OBJECT_WEIGHT: 0.0 
    CLASS_WEIGHT: 2.0
    MASK_WEIGHT: 5.0
    DICE_WEIGHT: 5.0
    HIDDEN_DIM: 256
    NUM_OBJECT_QUERIES: 100
    NHEADS: 8
    DROPOUT: 0.0
    DIM_FEEDFORWARD: 2048
    ENC_LAYERS: 0
    PRE_NORM: False
    ENFORCE_INPUT_PROJ: False
    SIZE_DIVISIBILITY: 32
    DEC_LAYERS: 10  # 9 decoder layers, add one for the loss on learnable query
    TRAIN_NUM_POINTS: 12544
    OVERSAMPLE_RATIO: 3.0
    IMPORTANCE_SAMPLE_RATIO: 0.75
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: False
      PANOPTIC_ON: False
      OVERLAP_THRESHOLD: 0.5
      OBJECT_MASK_THRESHOLD: 0.5
OUTPUT_DIR: ./output
DATASETS:
  TRAIN: ("ceiling_easy_train",)
  TEST: ("ceiling_easy_val",)
INPUT:
  DATASET_MAPPER_NAME: "ceiling_painting"

[05/15 07:39:04] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ceiling_easy_val
  TRAIN:
  - ceiling_easy_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: ceiling_painting
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  CLIP_MODEL:
    HEADS: 16
    INPUT_RESOLUTION: 336
    LAYERS: 24
    NAME: ViT-L/14@336px
    OUTPUT_DIM: 768
    PATCH_SIZE: 14
    TEMPERATURE: 0.01
    WIDTH: 1024
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.0
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: false
      OBJECT_MASK_THRESHOLD: 0.5
      OVERLAP_THRESHOLD: 0.5
      PANOPTIC_ON: false
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: true
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 4
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: ./mask_proposal_network.pth
OUTPUT_DIR: ./output
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 10000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 368751
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[05/15 07:39:04] detectron2 INFO: Full config saved to ./output/config.yaml
[05/15 07:39:04] d2.utils.env INFO: Using a generated random seed 5860272
[05/15 07:39:16] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0}
      num_classes: 4
      eos_coef: 0.0
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (maskclip): MaskCLIP(
    (visual): VisionTransformer(
      (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
      (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (12): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (13): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (14): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (15): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (16): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (17): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (18): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (19): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (20): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (21): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (22): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (23): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (conv1_added_params): Sequential(
          (0): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (1): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (2): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (3): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
        )
        (conv3_added_params): Sequential(
          (0): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (3): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
[05/15 07:39:16] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[05/15 07:39:16] d2.data.datasets.coco INFO: Loaded 118 images in COCO format from /home/ra78lof/consulting_pro/SAN/san/data/ceiling_painting_segmentation/train/json_annotation_train.json
[05/15 07:39:16] d2.data.build INFO: Removed 22 images with no usable annotations. 96 images left.
[05/15 07:39:16] d2.data.build INFO: Distribution of instances among all 4 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|   mural    | 0            |   brief    | 2            |   mural    | 338          |
|   relief   | 3            |            |              |            |              |
|   total    | 343          |            |              |            |              |[0m
[05/15 07:39:16] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[05/15 07:39:16] d2.data.build INFO: Using training sampler TrainingSampler
[05/15 07:39:16] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[05/15 07:39:16] d2.data.common INFO: Serializing 96 elements to byte tensors and concatenating them all ...
[05/15 07:39:16] d2.data.common INFO: Serialized dataset takes 1.02 MiB
[05/15 07:39:16] d2.data.build INFO: Making batched data loader with batch_size=8
[05/15 07:39:16] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[05/15 07:39:16] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from ./mask_proposal_network.pth ...
[05/15 07:39:16] fvcore.common.checkpoint INFO: [Checkpointer] Loading from ./mask_proposal_network.pth ...
[05/15 07:43:21] detectron2 INFO: Rank of current process: 0. World size: 1
[05/15 07:43:21] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0]
numpy                            2.2.5
detectron2                       0.6 @/home/ra78lof/consulting_pro/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.0
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   535.171.04
CUDA_HOME                        /usr
Pillow                           11.2.1
torchvision                      0.19.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.11.0
-------------------------------  ------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[05/15 07:43:21] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:57897', opts=[])
[05/15 07:43:21] detectron2 INFO: Contents of args.config_file=configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml:
_BASE_: Base-COCO-PanopticSegmentation.yaml
MODEL:
  MASK_ON: True
  WEIGHTS: "./mask_proposal_network.pth"
  META_ARCHITECTURE: "MaskFormer"
  SEM_SEG_HEAD:
    NAME: "MaskFormerHead"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 4
    LOSS_WEIGHT: 1.0
    CONVS_DIM: 256
    MASK_DIM: 256
    NORM: "GN"
    # pixel decoder
    PIXEL_DECODER_NAME: "MSDeformAttnPixelDecoder"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES: ["res3", "res4", "res5"]
    COMMON_STRIDE: 4
    TRANSFORMER_ENC_LAYERS: 6
  MASK_FORMER:
    TRANSFORMER_DECODER_NAME: "MultiScaleMaskedTransformerDecoder"
    TRANSFORMER_IN_FEATURE: "multi_scale_pixel_decoder"
    DEEP_SUPERVISION: True
    NO_OBJECT_WEIGHT: 0.0 
    CLASS_WEIGHT: 2.0
    MASK_WEIGHT: 5.0
    DICE_WEIGHT: 5.0
    HIDDEN_DIM: 256
    NUM_OBJECT_QUERIES: 100
    NHEADS: 8
    DROPOUT: 0.0
    DIM_FEEDFORWARD: 2048
    ENC_LAYERS: 0
    PRE_NORM: False
    ENFORCE_INPUT_PROJ: False
    SIZE_DIVISIBILITY: 32
    DEC_LAYERS: 10  # 9 decoder layers, add one for the loss on learnable query
    TRAIN_NUM_POINTS: 12544
    OVERSAMPLE_RATIO: 3.0
    IMPORTANCE_SAMPLE_RATIO: 0.75
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: False
      PANOPTIC_ON: False
      OVERLAP_THRESHOLD: 0.5
      OBJECT_MASK_THRESHOLD: 0.5
OUTPUT_DIR: ./output
DATASETS:
  TRAIN: ("ceiling_easy_train",)
  TEST: ("ceiling_easy_val",)
INPUT:
  DATASET_MAPPER_NAME: "ceiling_painting"

[05/15 07:43:21] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ceiling_easy_val
  TRAIN:
  - ceiling_easy_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: ceiling_painting
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  CLIP_MODEL:
    HEADS: 16
    INPUT_RESOLUTION: 336
    LAYERS: 24
    NAME: ViT-L/14@336px
    OUTPUT_DIM: 768
    PATCH_SIZE: 14
    TEMPERATURE: 0.01
    WIDTH: 1024
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.0
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: false
      OBJECT_MASK_THRESHOLD: 0.5
      OVERLAP_THRESHOLD: 0.5
      PANOPTIC_ON: false
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: true
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 4
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: ./mask_proposal_network.pth
OUTPUT_DIR: ./output
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 10000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 368751
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[05/15 07:43:21] detectron2 INFO: Full config saved to ./output/config.yaml
[05/15 07:43:21] d2.utils.env INFO: Using a generated random seed 22863835
[05/15 08:22:36] detectron2 INFO: Rank of current process: 0. World size: 1
[05/15 08:22:36] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0]
numpy                            2.2.5
detectron2                       0.6 @/home/ra78lof/consulting_pro/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.0
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   535.171.04
CUDA_HOME                        /usr
Pillow                           11.2.1
torchvision                      0.19.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.11.0
-------------------------------  ------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[05/15 08:22:36] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:57897', opts=[])
[05/15 08:22:36] detectron2 INFO: Contents of args.config_file=configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml:
_BASE_: Base-COCO-PanopticSegmentation.yaml
MODEL:
  MASK_ON: True
  WEIGHTS: "./mask_proposal_network.pth"
  META_ARCHITECTURE: "MaskFormer"
  SEM_SEG_HEAD:
    NAME: "MaskFormerHead"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 4
    LOSS_WEIGHT: 1.0
    CONVS_DIM: 256
    MASK_DIM: 256
    NORM: "GN"
    # pixel decoder
    PIXEL_DECODER_NAME: "MSDeformAttnPixelDecoder"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES: ["res3", "res4", "res5"]
    COMMON_STRIDE: 4
    TRANSFORMER_ENC_LAYERS: 6
  MASK_FORMER:
    TRANSFORMER_DECODER_NAME: "MultiScaleMaskedTransformerDecoder"
    TRANSFORMER_IN_FEATURE: "multi_scale_pixel_decoder"
    DEEP_SUPERVISION: True
    NO_OBJECT_WEIGHT: 0.0 
    CLASS_WEIGHT: 2.0
    MASK_WEIGHT: 5.0
    DICE_WEIGHT: 5.0
    HIDDEN_DIM: 256
    NUM_OBJECT_QUERIES: 100
    NHEADS: 8
    DROPOUT: 0.0
    DIM_FEEDFORWARD: 2048
    ENC_LAYERS: 0
    PRE_NORM: False
    ENFORCE_INPUT_PROJ: False
    SIZE_DIVISIBILITY: 32
    DEC_LAYERS: 10  # 9 decoder layers, add one for the loss on learnable query
    TRAIN_NUM_POINTS: 12544
    OVERSAMPLE_RATIO: 3.0
    IMPORTANCE_SAMPLE_RATIO: 0.75
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: False
      PANOPTIC_ON: False
      OVERLAP_THRESHOLD: 0.5
      OBJECT_MASK_THRESHOLD: 0.5
OUTPUT_DIR: ./output
DATASETS:
  TRAIN: ("ceiling_easy_train",)
  TEST: ("ceiling_easy_val",)
INPUT:
  DATASET_MAPPER_NAME: "ceiling_painting"

[05/15 08:22:36] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ceiling_easy_val
  TRAIN:
  - ceiling_easy_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: ceiling_painting
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  CLIP_MODEL:
    HEADS: 16
    INPUT_RESOLUTION: 336
    LAYERS: 24
    NAME: ViT-L/14@336px
    OUTPUT_DIM: 768
    PATCH_SIZE: 14
    TEMPERATURE: 0.01
    WIDTH: 1024
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.0
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: false
      OBJECT_MASK_THRESHOLD: 0.5
      OVERLAP_THRESHOLD: 0.5
      PANOPTIC_ON: false
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: true
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 4
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: ./mask_proposal_network.pth
OUTPUT_DIR: ./output
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 10000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 368751
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[05/15 08:22:36] detectron2 INFO: Full config saved to ./output/config.yaml
[05/15 08:22:36] d2.utils.env INFO: Using a generated random seed 37704748
[05/15 08:22:49] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0}
      num_classes: 4
      eos_coef: 0.0
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (maskclip): MaskCLIP(
    (visual): VisionTransformer(
      (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
      (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (12): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (13): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (14): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (15): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (16): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (17): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (18): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (19): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (20): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (21): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (22): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (23): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (conv1_added_params): Sequential(
          (0): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (1): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (2): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (3): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
        )
        (conv3_added_params): Sequential(
          (0): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (3): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
[05/15 08:22:49] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[05/15 08:22:49] d2.data.datasets.coco INFO: Loaded 118 images in COCO format from /home/ra78lof/consulting_pro/SAN/san/data/ceiling_painting_segmentation/train/json_annotation_train.json
[05/15 08:22:49] d2.data.build INFO: Removed 22 images with no usable annotations. 96 images left.
[05/15 08:22:49] d2.data.build INFO: Distribution of instances among all 4 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|   mural    | 0            |   brief    | 2            |   mural    | 338          |
|   relief   | 3            |            |              |            |              |
|   total    | 343          |            |              |            |              |[0m
[05/15 08:22:49] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[05/15 08:22:49] d2.data.build INFO: Using training sampler TrainingSampler
[05/15 08:22:49] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[05/15 08:22:49] d2.data.common INFO: Serializing 96 elements to byte tensors and concatenating them all ...
[05/15 08:22:49] d2.data.common INFO: Serialized dataset takes 1.02 MiB
[05/15 08:22:49] d2.data.build INFO: Making batched data loader with batch_size=8
[05/15 08:22:49] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[05/15 08:22:49] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from ./mask_proposal_network.pth ...
[05/15 08:22:49] fvcore.common.checkpoint INFO: [Checkpointer] Loading from ./mask_proposal_network.pth ...
[05/15 09:05:13] detectron2 INFO: Rank of current process: 0. World size: 1
[05/15 09:05:13] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0]
numpy                            2.2.5
detectron2                       0.6 @/home/ra78lof/consulting_pro/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.0
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   535.171.04
CUDA_HOME                        /usr
Pillow                           11.2.1
torchvision                      0.19.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.11.0
-------------------------------  ------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[05/15 09:05:13] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:57897', opts=[])
[05/15 09:05:13] detectron2 INFO: Contents of args.config_file=configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml:
_BASE_: Base-COCO-PanopticSegmentation.yaml
MODEL:
  MASK_ON: True
  WEIGHTS: "./mask_proposal_network.pth"
  META_ARCHITECTURE: "MaskFormer"
  SEM_SEG_HEAD:
    NAME: "MaskFormerHead"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 4
    LOSS_WEIGHT: 1.0
    CONVS_DIM: 256
    MASK_DIM: 256
    NORM: "GN"
    # pixel decoder
    PIXEL_DECODER_NAME: "MSDeformAttnPixelDecoder"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES: ["res3", "res4", "res5"]
    COMMON_STRIDE: 4
    TRANSFORMER_ENC_LAYERS: 6
  MASK_FORMER:
    TRANSFORMER_DECODER_NAME: "MultiScaleMaskedTransformerDecoder"
    TRANSFORMER_IN_FEATURE: "multi_scale_pixel_decoder"
    DEEP_SUPERVISION: True
    NO_OBJECT_WEIGHT: 0.0 
    CLASS_WEIGHT: 2.0
    MASK_WEIGHT: 5.0
    DICE_WEIGHT: 5.0
    HIDDEN_DIM: 256
    NUM_OBJECT_QUERIES: 100
    NHEADS: 8
    DROPOUT: 0.0
    DIM_FEEDFORWARD: 2048
    ENC_LAYERS: 0
    PRE_NORM: False
    ENFORCE_INPUT_PROJ: False
    SIZE_DIVISIBILITY: 32
    DEC_LAYERS: 10  # 9 decoder layers, add one for the loss on learnable query
    TRAIN_NUM_POINTS: 12544
    OVERSAMPLE_RATIO: 3.0
    IMPORTANCE_SAMPLE_RATIO: 0.75
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: False
      PANOPTIC_ON: False
      OVERLAP_THRESHOLD: 0.5
      OBJECT_MASK_THRESHOLD: 0.5
OUTPUT_DIR: ./output
DATASETS:
  TRAIN: ("ceiling_easy_train",)
  TEST: ("ceiling_easy_val",)
INPUT:
  DATASET_MAPPER_NAME: "ceiling_painting"

[05/15 09:05:13] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ceiling_easy_val
  TRAIN:
  - ceiling_easy_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: ceiling_painting
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  CLIP_MODEL:
    HEADS: 16
    INPUT_RESOLUTION: 336
    LAYERS: 24
    NAME: ViT-L/14@336px
    OUTPUT_DIM: 768
    PATCH_SIZE: 14
    TEMPERATURE: 0.01
    WIDTH: 1024
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.0
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: false
      OBJECT_MASK_THRESHOLD: 0.5
      OVERLAP_THRESHOLD: 0.5
      PANOPTIC_ON: false
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: true
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 4
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: ./mask_proposal_network.pth
OUTPUT_DIR: ./output
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 10000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 368751
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[05/15 09:05:13] detectron2 INFO: Full config saved to ./output/config.yaml
[05/15 09:05:13] d2.utils.env INFO: Using a generated random seed 14643002
[05/15 09:05:25] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0}
      num_classes: 4
      eos_coef: 0.0
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (maskclip): MaskCLIP(
    (visual): VisionTransformer(
      (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
      (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (12): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (13): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (14): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (15): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (16): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (17): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (18): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (19): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (20): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (21): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (22): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (23): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (conv1_added_params): Sequential(
          (0): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (1): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (2): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (3): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
        )
        (conv3_added_params): Sequential(
          (0): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (3): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
[05/15 09:05:25] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[05/15 09:05:25] d2.data.datasets.coco INFO: Loaded 118 images in COCO format from /home/ra78lof/consulting_pro/SAN/san/data/ceiling_painting_segmentation/train/json_annotation_train.json
[05/15 09:05:25] d2.data.build INFO: Removed 22 images with no usable annotations. 96 images left.
[05/15 09:05:25] d2.data.build INFO: Distribution of instances among all 4 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|   mural    | 0            |   brief    | 2            |   mural    | 338          |
|   relief   | 3            |            |              |            |              |
|   total    | 343          |            |              |            |              |[0m
[05/15 09:05:25] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[05/15 09:05:25] d2.data.build INFO: Using training sampler TrainingSampler
[05/15 09:05:25] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[05/15 09:05:25] d2.data.common INFO: Serializing 96 elements to byte tensors and concatenating them all ...
[05/15 09:05:25] d2.data.common INFO: Serialized dataset takes 1.02 MiB
[05/15 09:05:25] d2.data.build INFO: Making batched data loader with batch_size=8
[05/15 09:05:25] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[05/15 09:05:25] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from ./mask_proposal_network.pth ...
[05/15 09:05:25] fvcore.common.checkpoint INFO: [Checkpointer] Loading from ./mask_proposal_network.pth ...
[05/15 09:05:25] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (5,) in the model! You might want to double check if this is expected.
[05/15 09:05:25] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34mmaskclip.visual.conv1.weight[0m
[34mmaskclip.visual.ln_post.{bias, weight}[0m
[34mmaskclip.visual.ln_pre.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.0.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.1.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.2.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.3.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.0.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.1.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.2.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.3.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.{class_embedding, positional_embedding, proj}[0m
[05/15 09:05:25] d2.engine.train_loop INFO: Starting training from iteration 0
[05/15 09:05:26] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/ra78lof/consulting_pro/detectron2/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/ra78lof/consulting_pro/detectron2/detectron2/engine/defaults.py", line 530, in run_step
    self._trainer.run_step()
  File "/home/ra78lof/consulting_pro/detectron2/detectron2/engine/train_loop.py", line 494, in run_step
    loss_dict = self.model(data)
                ^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/consulting_pro/MaskCLIP/maskclip/model.py", line 277, in forward
    outputs = self.maskclip(images_clip, masks, self.txt_embed_train)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/consulting_pro/MaskCLIP/maskclip/modeling/maskclip.py", line 215, in forward
    img_fet, masks_list = self.visual(x, masks)
                          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/consulting_pro/MaskCLIP/maskclip/modeling/maskclip.py", line 170, in forward
    x, masks_list = self.transformer(x, masks)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/consulting_pro/MaskCLIP/maskclip/modeling/maskclip.py", line 115, in forward
    x, masks_res = block(x, masks, masks_embed)  # masks_res: Bx100x256x1024
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/consulting_pro/MaskCLIP/maskclip/modeling/maskclip.py", line 63, in forward
    x_res, masks_res = self.attention(self.ln_1(x), attn_mask=attn_mask, masks_embed=masks_embed)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/consulting_pro/MaskCLIP/maskclip/modeling/maskclip.py", line 47, in attention
    x, _, masks_res = self.attn(x, x, x, need_weights=False, attn_mask=attn_mask, masks_embed=masks_embed)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/consulting_pro/MaskCLIP/maskclip/modeling/attention.py", line 111, in forward
    masks_embed_q = self.q_prime_norm_added_params(masks_embed_q)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/modules/normalization.py", line 202, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/functional.py", line 2576, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.76 GiB. GPU 0 has a total capacity of 23.65 GiB of which 665.81 MiB is free. Including non-PyTorch memory, this process has 22.99 GiB memory in use. Of the allocated memory 21.57 GiB is allocated by PyTorch, and 987.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[05/15 09:05:26] d2.engine.hooks INFO: Total training time: 0:00:01 (0:00:00 on hooks)
[05/15 09:05:26] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 22084M
[05/15 09:06:10] detectron2 INFO: Rank of current process: 0. World size: 1
[05/15 09:06:10] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0]
numpy                            2.2.5
detectron2                       0.6 @/home/ra78lof/consulting_pro/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.0
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   535.171.04
CUDA_HOME                        /usr
Pillow                           11.2.1
torchvision                      0.19.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.11.0
-------------------------------  ------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[05/15 09:06:10] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:57897', opts=[])
[05/15 09:06:10] detectron2 INFO: Contents of args.config_file=configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml:
_BASE_: Base-COCO-PanopticSegmentation.yaml
MODEL:
  MASK_ON: True
  WEIGHTS: "./mask_proposal_network.pth"
  META_ARCHITECTURE: "MaskFormer"
  SEM_SEG_HEAD:
    NAME: "MaskFormerHead"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 4
    LOSS_WEIGHT: 1.0
    CONVS_DIM: 256
    MASK_DIM: 256
    NORM: "GN"
    # pixel decoder
    PIXEL_DECODER_NAME: "MSDeformAttnPixelDecoder"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES: ["res3", "res4", "res5"]
    COMMON_STRIDE: 4
    TRANSFORMER_ENC_LAYERS: 6
  MASK_FORMER:
    TRANSFORMER_DECODER_NAME: "MultiScaleMaskedTransformerDecoder"
    TRANSFORMER_IN_FEATURE: "multi_scale_pixel_decoder"
    DEEP_SUPERVISION: True
    NO_OBJECT_WEIGHT: 0.0 
    CLASS_WEIGHT: 2.0
    MASK_WEIGHT: 5.0
    DICE_WEIGHT: 5.0
    HIDDEN_DIM: 256
    NUM_OBJECT_QUERIES: 100
    NHEADS: 8
    DROPOUT: 0.0
    DIM_FEEDFORWARD: 2048
    ENC_LAYERS: 0
    PRE_NORM: False
    ENFORCE_INPUT_PROJ: False
    SIZE_DIVISIBILITY: 32
    DEC_LAYERS: 10  # 9 decoder layers, add one for the loss on learnable query
    TRAIN_NUM_POINTS: 12544
    OVERSAMPLE_RATIO: 3.0
    IMPORTANCE_SAMPLE_RATIO: 0.75
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: False
      PANOPTIC_ON: False
      OVERLAP_THRESHOLD: 0.5
      OBJECT_MASK_THRESHOLD: 0.5
OUTPUT_DIR: ./output
DATASETS:
  TRAIN: ("ceiling_easy_train",)
  TEST: ("ceiling_easy_val",)
INPUT:
  DATASET_MAPPER_NAME: "ceiling_painting"
SOLVER:
  IMS_PER_BATCH: 2
  MAX_ITER: 1000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.05
  STEPS: (327778, 355092)

[05/15 09:06:10] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ceiling_easy_val
  TRAIN:
  - ceiling_easy_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: ceiling_painting
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  CLIP_MODEL:
    HEADS: 16
    INPUT_RESOLUTION: 336
    LAYERS: 24
    NAME: ViT-L/14@336px
    OUTPUT_DIM: 768
    PATCH_SIZE: 14
    TEMPERATURE: 0.01
    WIDTH: 1024
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.0
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: false
      OBJECT_MASK_THRESHOLD: 0.5
      OVERLAP_THRESHOLD: 0.5
      PANOPTIC_ON: false
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: true
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 4
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: ./mask_proposal_network.pth
OUTPUT_DIR: ./output
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 368751
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[05/15 09:06:10] detectron2 INFO: Full config saved to ./output/config.yaml
[05/15 09:06:11] d2.utils.env INFO: Using a generated random seed 11987277
[05/15 09:06:22] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0}
      num_classes: 4
      eos_coef: 0.0
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (maskclip): MaskCLIP(
    (visual): VisionTransformer(
      (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
      (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (12): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (13): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (14): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (15): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (16): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (17): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (18): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (19): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (20): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (21): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (22): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (23): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (conv1_added_params): Sequential(
          (0): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (1): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (2): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (3): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
        )
        (conv3_added_params): Sequential(
          (0): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (3): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
[05/15 09:06:22] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[05/15 09:06:22] d2.data.datasets.coco INFO: Loaded 118 images in COCO format from /home/ra78lof/consulting_pro/SAN/san/data/ceiling_painting_segmentation/train/json_annotation_train.json
[05/15 09:06:22] d2.data.build INFO: Removed 22 images with no usable annotations. 96 images left.
[05/15 09:06:22] d2.data.build INFO: Distribution of instances among all 4 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|   mural    | 0            |   brief    | 2            |   mural    | 338          |
|   relief   | 3            |            |              |            |              |
|   total    | 343          |            |              |            |              |[0m
[05/15 09:06:22] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[05/15 09:06:22] d2.data.build INFO: Using training sampler TrainingSampler
[05/15 09:06:22] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[05/15 09:06:22] d2.data.common INFO: Serializing 96 elements to byte tensors and concatenating them all ...
[05/15 09:06:22] d2.data.common INFO: Serialized dataset takes 1.02 MiB
[05/15 09:06:22] d2.data.build INFO: Making batched data loader with batch_size=2
[05/15 09:06:22] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[05/15 09:06:22] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from ./mask_proposal_network.pth ...
[05/15 09:06:22] fvcore.common.checkpoint INFO: [Checkpointer] Loading from ./mask_proposal_network.pth ...
[05/15 09:06:22] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (5,) in the model! You might want to double check if this is expected.
[05/15 09:06:22] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34mmaskclip.visual.conv1.weight[0m
[34mmaskclip.visual.ln_post.{bias, weight}[0m
[34mmaskclip.visual.ln_pre.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.0.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.1.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.2.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.3.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.0.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.1.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.2.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.3.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.{class_embedding, positional_embedding, proj}[0m
[05/15 09:06:22] d2.engine.train_loop INFO: Starting training from iteration 0
[05/15 09:06:24] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/ra78lof/consulting_pro/detectron2/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/ra78lof/consulting_pro/detectron2/detectron2/engine/defaults.py", line 530, in run_step
    self._trainer.run_step()
  File "/home/ra78lof/consulting_pro/detectron2/detectron2/engine/train_loop.py", line 494, in run_step
    loss_dict = self.model(data)
                ^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/consulting_pro/MaskCLIP/maskclip/model.py", line 282, in forward
    targets = self.prepare_targets(gt_instances, images)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/consulting_pro/MaskCLIP/maskclip/model.py", line 350, in prepare_targets
    padded_masks = torch.zeros((gt_masks.shape[0], h_pad, w_pad), dtype=gt_masks.dtype, device=gt_masks.device)
                                ^^^^^^^^^^^^^^
AttributeError: 'PolygonMasks' object has no attribute 'shape'
[05/15 09:06:24] d2.engine.hooks INFO: Total training time: 0:00:01 (0:00:00 on hooks)
[05/15 09:06:24] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 17025M
[05/15 09:06:39] detectron2 INFO: Rank of current process: 0. World size: 1
[05/15 09:06:40] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0]
numpy                            2.2.5
detectron2                       0.6 @/home/ra78lof/consulting_pro/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.0
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   535.171.04
CUDA_HOME                        /usr
Pillow                           11.2.1
torchvision                      0.19.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.11.0
-------------------------------  ------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[05/15 09:06:40] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:57897', opts=[])
[05/15 09:06:40] detectron2 INFO: Contents of args.config_file=configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml:
_BASE_: Base-COCO-PanopticSegmentation.yaml
MODEL:
  MASK_ON: True
  WEIGHTS: "./mask_proposal_network.pth"
  META_ARCHITECTURE: "MaskFormer"
  SEM_SEG_HEAD:
    NAME: "MaskFormerHead"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 4
    LOSS_WEIGHT: 1.0
    CONVS_DIM: 256
    MASK_DIM: 256
    NORM: "GN"
    # pixel decoder
    PIXEL_DECODER_NAME: "MSDeformAttnPixelDecoder"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES: ["res3", "res4", "res5"]
    COMMON_STRIDE: 4
    TRANSFORMER_ENC_LAYERS: 6
  MASK_FORMER:
    TRANSFORMER_DECODER_NAME: "MultiScaleMaskedTransformerDecoder"
    TRANSFORMER_IN_FEATURE: "multi_scale_pixel_decoder"
    DEEP_SUPERVISION: True
    NO_OBJECT_WEIGHT: 0.0 
    CLASS_WEIGHT: 2.0
    MASK_WEIGHT: 5.0
    DICE_WEIGHT: 5.0
    HIDDEN_DIM: 256
    NUM_OBJECT_QUERIES: 100
    NHEADS: 8
    DROPOUT: 0.0
    DIM_FEEDFORWARD: 2048
    ENC_LAYERS: 0
    PRE_NORM: False
    ENFORCE_INPUT_PROJ: False
    SIZE_DIVISIBILITY: 32
    DEC_LAYERS: 10  # 9 decoder layers, add one for the loss on learnable query
    TRAIN_NUM_POINTS: 12544
    OVERSAMPLE_RATIO: 3.0
    IMPORTANCE_SAMPLE_RATIO: 0.75
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: False
      PANOPTIC_ON: False
      OVERLAP_THRESHOLD: 0.5
      OBJECT_MASK_THRESHOLD: 0.5
OUTPUT_DIR: ./output
DATASETS:
  TRAIN: ("ceiling_easy_train",)
  TEST: ("ceiling_easy_val",)
INPUT:
  DATASET_MAPPER_NAME: "ceiling_painting"
SOLVER:
  IMS_PER_BATCH: 2
  MAX_ITER: 1000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.05
  STEPS: (327778, 355092)

[05/15 09:06:40] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ceiling_easy_val
  TRAIN:
  - ceiling_easy_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: ceiling_painting
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  CLIP_MODEL:
    HEADS: 16
    INPUT_RESOLUTION: 336
    LAYERS: 24
    NAME: ViT-L/14@336px
    OUTPUT_DIM: 768
    PATCH_SIZE: 14
    TEMPERATURE: 0.01
    WIDTH: 1024
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.0
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: false
      OBJECT_MASK_THRESHOLD: 0.5
      OVERLAP_THRESHOLD: 0.5
      PANOPTIC_ON: false
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: true
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 4
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: ./mask_proposal_network.pth
OUTPUT_DIR: ./output
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 368751
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[05/15 09:06:40] detectron2 INFO: Full config saved to ./output/config.yaml
[05/15 09:06:40] d2.utils.env INFO: Using a generated random seed 41429893
[05/15 09:06:52] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0}
      num_classes: 4
      eos_coef: 0.0
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (maskclip): MaskCLIP(
    (visual): VisionTransformer(
      (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
      (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (12): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (13): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (14): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (15): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (16): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (17): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (18): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (19): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (20): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (21): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (22): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (23): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (conv1_added_params): Sequential(
          (0): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (1): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (2): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (3): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
        )
        (conv3_added_params): Sequential(
          (0): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (3): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
[05/15 09:06:52] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[05/15 09:06:52] d2.data.datasets.coco INFO: Loaded 118 images in COCO format from /home/ra78lof/consulting_pro/SAN/san/data/ceiling_painting_segmentation/train/json_annotation_train.json
[05/15 09:06:52] d2.data.build INFO: Removed 22 images with no usable annotations. 96 images left.
[05/15 09:06:52] d2.data.build INFO: Distribution of instances among all 4 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|   mural    | 0            |   brief    | 2            |   mural    | 338          |
|   relief   | 3            |            |              |            |              |
|   total    | 343          |            |              |            |              |[0m
[05/15 09:06:52] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[05/15 09:06:52] d2.data.build INFO: Using training sampler TrainingSampler
[05/15 09:06:52] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[05/15 09:06:52] d2.data.common INFO: Serializing 96 elements to byte tensors and concatenating them all ...
[05/15 09:06:52] d2.data.common INFO: Serialized dataset takes 1.02 MiB
[05/15 09:06:52] d2.data.build INFO: Making batched data loader with batch_size=2
[05/15 09:06:52] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[05/15 09:06:52] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from ./mask_proposal_network.pth ...
[05/15 09:06:52] fvcore.common.checkpoint INFO: [Checkpointer] Loading from ./mask_proposal_network.pth ...
[05/15 09:06:52] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (5,) in the model! You might want to double check if this is expected.
[05/15 09:06:52] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34mmaskclip.visual.conv1.weight[0m
[34mmaskclip.visual.ln_post.{bias, weight}[0m
[34mmaskclip.visual.ln_pre.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.0.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.1.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.2.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.3.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.0.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.1.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.2.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.3.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.{class_embedding, positional_embedding, proj}[0m
[05/15 09:06:52] d2.engine.train_loop INFO: Starting training from iteration 0
[05/15 09:06:53] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/ra78lof/consulting_pro/detectron2/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/ra78lof/consulting_pro/detectron2/detectron2/engine/defaults.py", line 530, in run_step
    self._trainer.run_step()
  File "/home/ra78lof/consulting_pro/detectron2/detectron2/engine/train_loop.py", line 494, in run_step
    loss_dict = self.model(data)
                ^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/consulting_pro/MaskCLIP/maskclip/model.py", line 282, in forward
    targets = self.prepare_targets(gt_instances, images)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/consulting_pro/MaskCLIP/maskclip/model.py", line 350, in prepare_targets
    padded_masks = torch.zeros((gt_masks.shape[0], h_pad, w_pad), dtype=gt_masks.dtype, device=gt_masks.device)
                                ^^^^^^^^^^^^^^
AttributeError: 'PolygonMasks' object has no attribute 'shape'
[05/15 09:06:53] d2.engine.hooks INFO: Total training time: 0:00:01 (0:00:00 on hooks)
[05/15 09:06:53] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 17025M
[05/15 09:08:37] detectron2 INFO: Rank of current process: 0. World size: 1
[05/15 09:08:37] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0]
numpy                            2.2.5
detectron2                       0.6 @/home/ra78lof/consulting_pro/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.0
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   535.171.04
CUDA_HOME                        /usr
Pillow                           11.2.1
torchvision                      0.19.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.11.0
-------------------------------  ------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[05/15 09:08:37] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:57897', opts=[])
[05/15 09:08:37] detectron2 INFO: Contents of args.config_file=configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml:
_BASE_: Base-COCO-PanopticSegmentation.yaml
MODEL:
  MASK_ON: True
  WEIGHTS: "./mask_proposal_network.pth"
  META_ARCHITECTURE: "MaskFormer"
  SEM_SEG_HEAD:
    NAME: "MaskFormerHead"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 4
    LOSS_WEIGHT: 1.0
    CONVS_DIM: 256
    MASK_DIM: 256
    NORM: "GN"
    # pixel decoder
    PIXEL_DECODER_NAME: "MSDeformAttnPixelDecoder"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES: ["res3", "res4", "res5"]
    COMMON_STRIDE: 4
    TRANSFORMER_ENC_LAYERS: 6
  MASK_FORMER:
    TRANSFORMER_DECODER_NAME: "MultiScaleMaskedTransformerDecoder"
    TRANSFORMER_IN_FEATURE: "multi_scale_pixel_decoder"
    DEEP_SUPERVISION: True
    NO_OBJECT_WEIGHT: 0.0 
    CLASS_WEIGHT: 2.0
    MASK_WEIGHT: 5.0
    DICE_WEIGHT: 5.0
    HIDDEN_DIM: 256
    NUM_OBJECT_QUERIES: 100
    NHEADS: 8
    DROPOUT: 0.0
    DIM_FEEDFORWARD: 2048
    ENC_LAYERS: 0
    PRE_NORM: False
    ENFORCE_INPUT_PROJ: False
    SIZE_DIVISIBILITY: 32
    DEC_LAYERS: 10  # 9 decoder layers, add one for the loss on learnable query
    TRAIN_NUM_POINTS: 12544
    OVERSAMPLE_RATIO: 3.0
    IMPORTANCE_SAMPLE_RATIO: 0.75
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: False
      PANOPTIC_ON: False
      OVERLAP_THRESHOLD: 0.5
      OBJECT_MASK_THRESHOLD: 0.5
OUTPUT_DIR: ./output
DATASETS:
  TRAIN: ("ceiling_easy_train",)
  TEST: ("ceiling_easy_val",)
INPUT:
  DATASET_MAPPER_NAME: "ceiling_painting"
SOLVER:
  IMS_PER_BATCH: 2
  MAX_ITER: 1000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.05
  STEPS: (327778, 355092)

[05/15 09:08:37] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ceiling_easy_val
  TRAIN:
  - ceiling_easy_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: ceiling_painting
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  CLIP_MODEL:
    HEADS: 16
    INPUT_RESOLUTION: 336
    LAYERS: 24
    NAME: ViT-L/14@336px
    OUTPUT_DIM: 768
    PATCH_SIZE: 14
    TEMPERATURE: 0.01
    WIDTH: 1024
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.0
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: false
      OBJECT_MASK_THRESHOLD: 0.5
      OVERLAP_THRESHOLD: 0.5
      PANOPTIC_ON: false
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: true
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 4
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: ./mask_proposal_network.pth
OUTPUT_DIR: ./output
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 368751
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[05/15 09:08:37] detectron2 INFO: Full config saved to ./output/config.yaml
[05/15 09:08:37] d2.utils.env INFO: Using a generated random seed 39010567
[05/15 09:08:49] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0}
      num_classes: 4
      eos_coef: 0.0
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (maskclip): MaskCLIP(
    (visual): VisionTransformer(
      (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
      (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (12): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (13): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (14): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (15): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (16): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (17): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (18): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (19): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (20): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (21): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (22): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (23): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (conv1_added_params): Sequential(
          (0): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (1): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (2): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (3): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
        )
        (conv3_added_params): Sequential(
          (0): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (3): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
[05/15 09:08:50] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[05/15 09:08:50] d2.data.datasets.coco INFO: Loaded 118 images in COCO format from /home/ra78lof/consulting_pro/SAN/san/data/ceiling_painting_segmentation/train/json_annotation_train.json
[05/15 09:08:50] d2.data.build INFO: Removed 22 images with no usable annotations. 96 images left.
[05/15 09:08:50] d2.data.build INFO: Distribution of instances among all 4 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|   mural    | 0            |   brief    | 2            |   mural    | 338          |
|   relief   | 3            |            |              |            |              |
|   total    | 343          |            |              |            |              |[0m
[05/15 09:08:50] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[05/15 09:08:50] d2.data.build INFO: Using training sampler TrainingSampler
[05/15 09:08:50] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[05/15 09:08:50] d2.data.common INFO: Serializing 96 elements to byte tensors and concatenating them all ...
[05/15 09:08:50] d2.data.common INFO: Serialized dataset takes 1.02 MiB
[05/15 09:08:50] d2.data.build INFO: Making batched data loader with batch_size=2
[05/15 09:08:50] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[05/15 09:08:50] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from ./mask_proposal_network.pth ...
[05/15 09:08:50] fvcore.common.checkpoint INFO: [Checkpointer] Loading from ./mask_proposal_network.pth ...
[05/15 09:08:50] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (5,) in the model! You might want to double check if this is expected.
[05/15 09:08:50] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34mmaskclip.visual.conv1.weight[0m
[34mmaskclip.visual.ln_post.{bias, weight}[0m
[34mmaskclip.visual.ln_pre.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.0.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.1.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.2.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.3.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.0.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.1.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.2.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.3.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.{class_embedding, positional_embedding, proj}[0m
[05/15 09:08:50] d2.engine.train_loop INFO: Starting training from iteration 0
[05/15 09:08:51] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/ra78lof/consulting_pro/detectron2/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/ra78lof/consulting_pro/detectron2/detectron2/engine/defaults.py", line 530, in run_step
    self._trainer.run_step()
  File "/home/ra78lof/consulting_pro/detectron2/detectron2/engine/train_loop.py", line 494, in run_step
    loss_dict = self.model(data)
                ^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/consulting_pro/MaskCLIP/maskclip/model.py", line 282, in forward
    targets = self.prepare_targets(gt_instances, images)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/consulting_pro/MaskCLIP/maskclip/model.py", line 350, in prepare_targets
    gt_masks = BitMasks.from_polygon_masks(gt_masks.polygons, h, w)
                                                              ^
NameError: name 'h' is not defined
[05/15 09:08:51] d2.engine.hooks INFO: Total training time: 0:00:01 (0:00:00 on hooks)
[05/15 09:08:51] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 17025M
[05/15 09:09:08] detectron2 INFO: Rank of current process: 0. World size: 1
[05/15 09:09:08] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0]
numpy                            2.2.5
detectron2                       0.6 @/home/ra78lof/consulting_pro/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.0
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   535.171.04
CUDA_HOME                        /usr
Pillow                           11.2.1
torchvision                      0.19.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.11.0
-------------------------------  ------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[05/15 09:09:08] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:57897', opts=[])
[05/15 09:09:08] detectron2 INFO: Contents of args.config_file=configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml:
_BASE_: Base-COCO-PanopticSegmentation.yaml
MODEL:
  MASK_ON: True
  WEIGHTS: "./mask_proposal_network.pth"
  META_ARCHITECTURE: "MaskFormer"
  SEM_SEG_HEAD:
    NAME: "MaskFormerHead"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 4
    LOSS_WEIGHT: 1.0
    CONVS_DIM: 256
    MASK_DIM: 256
    NORM: "GN"
    # pixel decoder
    PIXEL_DECODER_NAME: "MSDeformAttnPixelDecoder"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES: ["res3", "res4", "res5"]
    COMMON_STRIDE: 4
    TRANSFORMER_ENC_LAYERS: 6
  MASK_FORMER:
    TRANSFORMER_DECODER_NAME: "MultiScaleMaskedTransformerDecoder"
    TRANSFORMER_IN_FEATURE: "multi_scale_pixel_decoder"
    DEEP_SUPERVISION: True
    NO_OBJECT_WEIGHT: 0.0 
    CLASS_WEIGHT: 2.0
    MASK_WEIGHT: 5.0
    DICE_WEIGHT: 5.0
    HIDDEN_DIM: 256
    NUM_OBJECT_QUERIES: 100
    NHEADS: 8
    DROPOUT: 0.0
    DIM_FEEDFORWARD: 2048
    ENC_LAYERS: 0
    PRE_NORM: False
    ENFORCE_INPUT_PROJ: False
    SIZE_DIVISIBILITY: 32
    DEC_LAYERS: 10  # 9 decoder layers, add one for the loss on learnable query
    TRAIN_NUM_POINTS: 12544
    OVERSAMPLE_RATIO: 3.0
    IMPORTANCE_SAMPLE_RATIO: 0.75
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: False
      PANOPTIC_ON: False
      OVERLAP_THRESHOLD: 0.5
      OBJECT_MASK_THRESHOLD: 0.5
OUTPUT_DIR: ./output
DATASETS:
  TRAIN: ("ceiling_easy_train",)
  TEST: ("ceiling_easy_val",)
INPUT:
  DATASET_MAPPER_NAME: "ceiling_painting"
SOLVER:
  IMS_PER_BATCH: 2
  MAX_ITER: 1000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.05
  STEPS: (327778, 355092)

[05/15 09:09:08] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ceiling_easy_val
  TRAIN:
  - ceiling_easy_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: ceiling_painting
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  CLIP_MODEL:
    HEADS: 16
    INPUT_RESOLUTION: 336
    LAYERS: 24
    NAME: ViT-L/14@336px
    OUTPUT_DIM: 768
    PATCH_SIZE: 14
    TEMPERATURE: 0.01
    WIDTH: 1024
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.0
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: false
      OBJECT_MASK_THRESHOLD: 0.5
      OVERLAP_THRESHOLD: 0.5
      PANOPTIC_ON: false
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: true
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 4
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: ./mask_proposal_network.pth
OUTPUT_DIR: ./output
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 368751
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[05/15 09:09:08] detectron2 INFO: Full config saved to ./output/config.yaml
[05/15 09:09:08] d2.utils.env INFO: Using a generated random seed 9983078
[05/15 09:09:21] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0}
      num_classes: 4
      eos_coef: 0.0
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (maskclip): MaskCLIP(
    (visual): VisionTransformer(
      (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
      (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (12): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (13): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (14): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (15): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (16): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (17): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (18): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (19): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (20): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (21): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (22): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (23): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (conv1_added_params): Sequential(
          (0): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (1): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (2): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (3): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
        )
        (conv3_added_params): Sequential(
          (0): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (3): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
[05/15 09:09:21] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[05/15 09:09:21] d2.data.datasets.coco INFO: Loaded 118 images in COCO format from /home/ra78lof/consulting_pro/SAN/san/data/ceiling_painting_segmentation/train/json_annotation_train.json
[05/15 09:09:21] d2.data.build INFO: Removed 22 images with no usable annotations. 96 images left.
[05/15 09:09:21] d2.data.build INFO: Distribution of instances among all 4 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|   mural    | 0            |   brief    | 2            |   mural    | 338          |
|   relief   | 3            |            |              |            |              |
|   total    | 343          |            |              |            |              |[0m
[05/15 09:09:21] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[05/15 09:09:21] d2.data.build INFO: Using training sampler TrainingSampler
[05/15 09:09:21] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[05/15 09:09:21] d2.data.common INFO: Serializing 96 elements to byte tensors and concatenating them all ...
[05/15 09:09:21] d2.data.common INFO: Serialized dataset takes 1.02 MiB
[05/15 09:09:21] d2.data.build INFO: Making batched data loader with batch_size=2
[05/15 09:09:21] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[05/15 09:09:21] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from ./mask_proposal_network.pth ...
[05/15 09:09:21] fvcore.common.checkpoint INFO: [Checkpointer] Loading from ./mask_proposal_network.pth ...
[05/15 09:09:21] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (5,) in the model! You might want to double check if this is expected.
[05/15 09:09:21] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34mmaskclip.visual.conv1.weight[0m
[34mmaskclip.visual.ln_post.{bias, weight}[0m
[34mmaskclip.visual.ln_pre.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.0.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.1.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.2.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.3.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.0.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.1.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.2.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.3.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.{class_embedding, positional_embedding, proj}[0m
[05/15 09:09:21] d2.engine.train_loop INFO: Starting training from iteration 0
[05/15 09:09:22] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/ra78lof/consulting_pro/detectron2/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/ra78lof/consulting_pro/detectron2/detectron2/engine/defaults.py", line 530, in run_step
    self._trainer.run_step()
  File "/home/ra78lof/consulting_pro/detectron2/detectron2/engine/train_loop.py", line 494, in run_step
    loss_dict = self.model(data)
                ^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/consulting_pro/MaskCLIP/maskclip/model.py", line 282, in forward
    targets = self.prepare_targets(gt_instances, images)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ra78lof/consulting_pro/MaskCLIP/maskclip/model.py", line 349, in prepare_targets
    h, w = targets_per_image.gt_masks.shape[-2:]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'PolygonMasks' object has no attribute 'shape'
[05/15 09:09:22] d2.engine.hooks INFO: Total training time: 0:00:01 (0:00:00 on hooks)
[05/15 09:09:22] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 17025M
[05/15 09:10:29] detectron2 INFO: Rank of current process: 0. World size: 1
[05/15 09:10:29] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0]
numpy                            2.2.5
detectron2                       0.6 @/home/ra78lof/consulting_pro/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.0
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   535.171.04
CUDA_HOME                        /usr
Pillow                           11.2.1
torchvision                      0.19.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.11.0
-------------------------------  ------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[05/15 09:10:29] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:57897', opts=[])
[05/15 09:10:29] detectron2 INFO: Contents of args.config_file=configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml:
_BASE_: Base-COCO-PanopticSegmentation.yaml
MODEL:
  MASK_ON: True
  WEIGHTS: "./mask_proposal_network.pth"
  META_ARCHITECTURE: "MaskFormer"
  SEM_SEG_HEAD:
    NAME: "MaskFormerHead"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 4
    LOSS_WEIGHT: 1.0
    CONVS_DIM: 256
    MASK_DIM: 256
    NORM: "GN"
    # pixel decoder
    PIXEL_DECODER_NAME: "MSDeformAttnPixelDecoder"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES: ["res3", "res4", "res5"]
    COMMON_STRIDE: 4
    TRANSFORMER_ENC_LAYERS: 6
  MASK_FORMER:
    TRANSFORMER_DECODER_NAME: "MultiScaleMaskedTransformerDecoder"
    TRANSFORMER_IN_FEATURE: "multi_scale_pixel_decoder"
    DEEP_SUPERVISION: True
    NO_OBJECT_WEIGHT: 0.0 
    CLASS_WEIGHT: 2.0
    MASK_WEIGHT: 5.0
    DICE_WEIGHT: 5.0
    HIDDEN_DIM: 256
    NUM_OBJECT_QUERIES: 100
    NHEADS: 8
    DROPOUT: 0.0
    DIM_FEEDFORWARD: 2048
    ENC_LAYERS: 0
    PRE_NORM: False
    ENFORCE_INPUT_PROJ: False
    SIZE_DIVISIBILITY: 32
    DEC_LAYERS: 10  # 9 decoder layers, add one for the loss on learnable query
    TRAIN_NUM_POINTS: 12544
    OVERSAMPLE_RATIO: 3.0
    IMPORTANCE_SAMPLE_RATIO: 0.75
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: False
      PANOPTIC_ON: False
      OVERLAP_THRESHOLD: 0.5
      OBJECT_MASK_THRESHOLD: 0.5
OUTPUT_DIR: ./output
DATASETS:
  TRAIN: ("ceiling_easy_train",)
  TEST: ("ceiling_easy_val",)
INPUT:
  DATASET_MAPPER_NAME: "ceiling_painting"
SOLVER:
  IMS_PER_BATCH: 2
  MAX_ITER: 1000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.05
  STEPS: (327778, 355092)

[05/15 09:10:29] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ceiling_easy_val
  TRAIN:
  - ceiling_easy_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: ceiling_painting
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  CLIP_MODEL:
    HEADS: 16
    INPUT_RESOLUTION: 336
    LAYERS: 24
    NAME: ViT-L/14@336px
    OUTPUT_DIM: 768
    PATCH_SIZE: 14
    TEMPERATURE: 0.01
    WIDTH: 1024
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.0
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: false
      OBJECT_MASK_THRESHOLD: 0.5
      OVERLAP_THRESHOLD: 0.5
      PANOPTIC_ON: false
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: true
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 4
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: ./mask_proposal_network.pth
OUTPUT_DIR: ./output
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 368751
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[05/15 09:10:29] detectron2 INFO: Full config saved to ./output/config.yaml
[05/15 09:10:29] d2.utils.env INFO: Using a generated random seed 30540281
[05/15 09:10:41] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0}
      num_classes: 4
      eos_coef: 0.0
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (maskclip): MaskCLIP(
    (visual): VisionTransformer(
      (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
      (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (12): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (13): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (14): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (15): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (16): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (17): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (18): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (19): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (20): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (21): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (22): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (23): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (conv1_added_params): Sequential(
          (0): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (1): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (2): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (3): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
        )
        (conv3_added_params): Sequential(
          (0): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (3): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
[05/15 09:10:41] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[05/15 09:10:41] d2.data.datasets.coco INFO: Loaded 118 images in COCO format from /home/ra78lof/consulting_pro/SAN/san/data/ceiling_painting_segmentation/train/json_annotation_train.json
[05/15 09:10:41] d2.data.build INFO: Removed 22 images with no usable annotations. 96 images left.
[05/15 09:10:41] d2.data.build INFO: Distribution of instances among all 4 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|   mural    | 0            |   brief    | 2            |   mural    | 338          |
|   relief   | 3            |            |              |            |              |
|   total    | 343          |            |              |            |              |[0m
[05/15 09:10:41] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[05/15 09:10:41] d2.data.build INFO: Using training sampler TrainingSampler
[05/15 09:10:41] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[05/15 09:10:41] d2.data.common INFO: Serializing 96 elements to byte tensors and concatenating them all ...
[05/15 09:10:41] d2.data.common INFO: Serialized dataset takes 1.02 MiB
[05/15 09:10:41] d2.data.build INFO: Making batched data loader with batch_size=2
[05/15 09:10:41] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[05/15 09:10:41] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from ./mask_proposal_network.pth ...
[05/15 09:10:41] fvcore.common.checkpoint INFO: [Checkpointer] Loading from ./mask_proposal_network.pth ...
[05/15 09:10:41] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (5,) in the model! You might want to double check if this is expected.
[05/15 09:10:41] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34mmaskclip.visual.conv1.weight[0m
[34mmaskclip.visual.ln_post.{bias, weight}[0m
[34mmaskclip.visual.ln_pre.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.0.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.1.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.2.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.3.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.0.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.1.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.2.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.3.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.{class_embedding, positional_embedding, proj}[0m
[05/15 09:10:41] d2.engine.train_loop INFO: Starting training from iteration 0
[05/15 09:10:47] d2.engine.hooks INFO: Overall training speed: 3 iterations in 0:00:02 (0.9051 s / it)
[05/15 09:10:47] d2.engine.hooks INFO: Total training time: 0:00:02 (0:00:00 on hooks)
[05/15 09:10:47] d2.utils.events INFO:  eta: 0:13:07  iter: 5  total_loss: 9.888  loss_ce: 4.819  loss_mask: 0.5606  loss_dice: 0.7363  loss_mask_0: 0.5662  loss_dice_0: 0.73  loss_mask_1: 0.5677  loss_dice_1: 0.7449  loss_mask_2: 0.5475  loss_dice_2: 0.726    time: 0.7330  last_time: 0.7925  data_time: 0.0643  last_data_time: 0.0010   lr: 0.0001  max_mem: 17028M
[05/15 09:12:06] detectron2 INFO: Rank of current process: 0. World size: 1
[05/15 09:12:07] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0]
numpy                            2.2.5
detectron2                       0.6 @/home/ra78lof/consulting_pro/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.0
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   535.171.04
CUDA_HOME                        /usr
Pillow                           11.2.1
torchvision                      0.19.0+cu121 @/home/ra78lof/anaconda3/envs/fcclip/lib/python3.11/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.11.0
-------------------------------  ------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[05/15 09:12:07] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:57897', opts=[])
[05/15 09:12:07] detectron2 INFO: Contents of args.config_file=configs/coco/ceiling_maskformer2_R50_bs2_10ep.yaml:
_BASE_: Base-COCO-PanopticSegmentation.yaml
MODEL:
  MASK_ON: True
  WEIGHTS: "./mask_proposal_network.pth"
  META_ARCHITECTURE: "MaskFormer"
  SEM_SEG_HEAD:
    NAME: "MaskFormerHead"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    IGNORE_VALUE: 255
    NUM_CLASSES: 4
    LOSS_WEIGHT: 1.0
    CONVS_DIM: 256
    MASK_DIM: 256
    NORM: "GN"
    # pixel decoder
    PIXEL_DECODER_NAME: "MSDeformAttnPixelDecoder"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES: ["res3", "res4", "res5"]
    COMMON_STRIDE: 4
    TRANSFORMER_ENC_LAYERS: 6
  MASK_FORMER:
    TRANSFORMER_DECODER_NAME: "MultiScaleMaskedTransformerDecoder"
    TRANSFORMER_IN_FEATURE: "multi_scale_pixel_decoder"
    DEEP_SUPERVISION: True
    NO_OBJECT_WEIGHT: 0.0 
    CLASS_WEIGHT: 2.0
    MASK_WEIGHT: 5.0
    DICE_WEIGHT: 5.0
    HIDDEN_DIM: 256
    NUM_OBJECT_QUERIES: 100
    NHEADS: 8
    DROPOUT: 0.0
    DIM_FEEDFORWARD: 2048
    ENC_LAYERS: 0
    PRE_NORM: False
    ENFORCE_INPUT_PROJ: False
    SIZE_DIVISIBILITY: 32
    DEC_LAYERS: 10  # 9 decoder layers, add one for the loss on learnable query
    TRAIN_NUM_POINTS: 12544
    OVERSAMPLE_RATIO: 3.0
    IMPORTANCE_SAMPLE_RATIO: 0.75
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: False
      PANOPTIC_ON: False
      OVERLAP_THRESHOLD: 0.5
      OBJECT_MASK_THRESHOLD: 0.5
OUTPUT_DIR: ./output
DATASETS:
  TRAIN: ("ceiling_easy_train",)
  TEST: ("ceiling_easy_val",)
INPUT:
  DATASET_MAPPER_NAME: "ceiling_painting"
SOLVER:
  IMS_PER_BATCH: 2
  MAX_ITER: 1000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WEIGHT_DECAY: 0.05
  STEPS: (327778, 355092)

[05/15 09:12:07] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ceiling_easy_val
  TRAIN:
  - ceiling_easy_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: ceiling_painting
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  CLIP_MODEL:
    HEADS: 16
    INPUT_RESOLUTION: 336
    LAYERS: 24
    NAME: ViT-L/14@336px
    OUTPUT_DIM: 768
    PATCH_SIZE: 14
    TEMPERATURE: 0.01
    WIDTH: 1024
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.0
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: false
      OBJECT_MASK_THRESHOLD: 0.5
      OVERLAP_THRESHOLD: 0.5
      PANOPTIC_ON: false
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: true
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 4
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: ./mask_proposal_network.pth
OUTPUT_DIR: ./output
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 368751
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[05/15 09:12:07] detectron2 INFO: Full config saved to ./output/config.yaml
[05/15 09:12:07] d2.utils.env INFO: Using a generated random seed 8235727
[05/15 09:12:19] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0}
      num_classes: 4
      eos_coef: 0.0
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (maskclip): MaskCLIP(
    (visual): VisionTransformer(
      (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
      (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (12): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (13): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (14): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (15): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (16): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (17): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (18): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (19): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (20): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (21): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (22): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (23): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_prime_norm_added_params): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (k_prime_proj_weight_added_params): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (conv1_added_params): Sequential(
          (0): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (1): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (2): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
          (3): Conv2d(1, 1024, kernel_size=(14, 14), stride=(14, 14))
        )
        (conv3_added_params): Sequential(
          (0): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
          (3): Conv2d(1024, 196, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
[05/15 09:12:19] d2.data.datasets.coco WARNING: 
Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.

[05/15 09:12:19] d2.data.datasets.coco INFO: Loaded 118 images in COCO format from /home/ra78lof/consulting_pro/SAN/san/data/ceiling_painting_segmentation/train/json_annotation_train.json
[05/15 09:12:19] d2.data.build INFO: Removed 22 images with no usable annotations. 96 images left.
[05/15 09:12:19] d2.data.build INFO: Distribution of instances among all 4 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|   mural    | 0            |   brief    | 2            |   mural    | 338          |
|   relief   | 3            |            |              |            |              |
|   total    | 343          |            |              |            |              |[0m
[05/15 09:12:19] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[05/15 09:12:19] d2.data.build INFO: Using training sampler TrainingSampler
[05/15 09:12:19] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[05/15 09:12:19] d2.data.common INFO: Serializing 96 elements to byte tensors and concatenating them all ...
[05/15 09:12:19] d2.data.common INFO: Serialized dataset takes 1.02 MiB
[05/15 09:12:19] d2.data.build INFO: Making batched data loader with batch_size=2
[05/15 09:12:19] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[05/15 09:12:19] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from ./mask_proposal_network.pth ...
[05/15 09:12:19] fvcore.common.checkpoint INFO: [Checkpointer] Loading from ./mask_proposal_network.pth ...
[05/15 09:12:19] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (5,) in the model! You might want to double check if this is expected.
[05/15 09:12:19] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34mmaskclip.visual.conv1.weight[0m
[34mmaskclip.visual.ln_post.{bias, weight}[0m
[34mmaskclip.visual.ln_pre.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.0.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.1.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.2.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv1_added_params.3.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.0.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.1.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.2.{bias, weight}[0m
[34mmaskclip.visual.transformer.conv3_added_params.3.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.12.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.13.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.14.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.15.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.16.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.17.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.18.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.19.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.20.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.21.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.22.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.23.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.k_prime_proj_weight_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.q_prime_norm_added_params.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mmaskclip.visual.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mmaskclip.visual.{class_embedding, positional_embedding, proj}[0m
[05/15 09:12:19] d2.engine.train_loop INFO: Starting training from iteration 0
[05/15 09:12:33] d2.utils.events INFO:  eta: 0:09:15  iter: 19  total_loss: 12.89  loss_ce: 5.243  loss_mask: 0.8122  loss_dice: 0.9567  loss_mask_0: 0.773  loss_dice_0: 0.9549  loss_mask_1: 0.7734  loss_dice_1: 0.9736  loss_mask_2: 0.7946  loss_dice_2: 0.9631    time: 0.6091  last_time: 0.6564  data_time: 0.0167  last_data_time: 0.0026   lr: 0.0001  max_mem: 17075M
[05/15 09:12:45] d2.utils.events INFO:  eta: 0:09:10  iter: 39  total_loss: 13.58  loss_ce: 2.079  loss_mask: 0.9415  loss_dice: 1.795  loss_mask_0: 0.8815  loss_dice_0: 1.777  loss_mask_1: 0.9087  loss_dice_1: 1.788  loss_mask_2: 0.911  loss_dice_2: 1.772    time: 0.6010  last_time: 0.5762  data_time: 0.0027  last_data_time: 0.0026   lr: 0.0001  max_mem: 17075M
[05/15 09:12:57] d2.utils.events INFO:  eta: 0:09:01  iter: 59  total_loss: 12.91  loss_ce: 1.286  loss_mask: 1.122  loss_dice: 1.96  loss_mask_0: 1.123  loss_dice_0: 1.916  loss_mask_1: 1.132  loss_dice_1: 1.905  loss_mask_2: 1.119  loss_dice_2: 1.89    time: 0.5958  last_time: 0.5588  data_time: 0.0025  last_data_time: 0.0025   lr: 0.0001  max_mem: 17075M
[05/15 09:13:09] d2.utils.events INFO:  eta: 0:08:50  iter: 79  total_loss: 8.862  loss_ce: 0.4034  loss_mask: 0.8105  loss_dice: 1.379  loss_mask_0: 0.813  loss_dice_0: 1.385  loss_mask_1: 0.8018  loss_dice_1: 1.369  loss_mask_2: 0.8016  loss_dice_2: 1.368    time: 0.5950  last_time: 0.5564  data_time: 0.0025  last_data_time: 0.0022   lr: 0.0001  max_mem: 17075M
[05/15 09:13:20] d2.utils.events INFO:  eta: 0:08:38  iter: 99  total_loss: 10.18  loss_ce: 0.2757  loss_mask: 0.8006  loss_dice: 1.482  loss_mask_0: 0.8102  loss_dice_0: 1.46  loss_mask_1: 0.8054  loss_dice_1: 1.456  loss_mask_2: 0.7906  loss_dice_2: 1.448    time: 0.5937  last_time: 0.6672  data_time: 0.0027  last_data_time: 0.0026   lr: 0.0001  max_mem: 17075M
[05/15 09:13:23] d2.engine.hooks INFO: Overall training speed: 101 iterations in 0:01:00 (0.5984 s / it)
[05/15 09:13:23] d2.engine.hooks INFO: Total training time: 0:01:00 (0:00:00 on hooks)
[05/15 09:13:23] d2.utils.events INFO:  eta: 0:08:36  iter: 103  total_loss: 10.18  loss_ce: 0.237  loss_mask: 0.9563  loss_dice: 1.418  loss_mask_0: 0.9589  loss_dice_0: 1.404  loss_mask_1: 0.9536  loss_dice_1: 1.432  loss_mask_2: 0.9487  loss_dice_2: 1.412    time: 0.5933  last_time: 0.5641  data_time: 0.0026  last_data_time: 0.0024   lr: 0.0001  max_mem: 17075M
